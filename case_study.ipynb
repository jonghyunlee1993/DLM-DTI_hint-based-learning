{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b8b3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug_name</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Target_name</th>\n",
       "      <th>FASTA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phenacetin</td>\n",
       "      <td>CCOC1=CC=C(C=C1)NC(=O)C</td>\n",
       "      <td>1A2</td>\n",
       "      <td>MAVLKGLRPRVPKGLKSPPEPWGWPLLGHVLTLGKNPHLALSRMSQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7-Ethoxyresorufin</td>\n",
       "      <td>CCOC1=CC2=C(C=C1)N=C3C=CC(=O)C=C3O2</td>\n",
       "      <td>1A2</td>\n",
       "      <td>MAVLKGLRPRVPKGLKSPPEPWGWPLLGHVLTLGKNPHLALSRMSQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Efavirenz</td>\n",
       "      <td>C1CC1C#CC2(C3=C(C=CC(=C3)Cl)NC(=O)O2)C(F)(F)F</td>\n",
       "      <td>2B6</td>\n",
       "      <td>MAKKTSSKGKLPPGPRPLPLLGNLLQMDRRGLLKSFLRFREKYGDV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bupropion</td>\n",
       "      <td>CC(C(=O)C1=CC(=CC=C1)Cl)NC(C)(C)C\\n</td>\n",
       "      <td>2B6</td>\n",
       "      <td>MAKKTSSKGKLPPGPRPLPLLGNLLQMDRRGLLKSFLRFREKYGDV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Midazolam</td>\n",
       "      <td>CC1=NC=C2N1C3=C(C=C(C=C3)Cl)C(=NC2)C4=CC=CC=C4F</td>\n",
       "      <td>3A4</td>\n",
       "      <td>MAYLYGTHSHGLFKKLGIPGPTPLPFLGNILSYHKGFCMFDMECHK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Testosterone</td>\n",
       "      <td>CC12CCC3C(C1CCC2O)CCC4=CC(=O)CCC34C</td>\n",
       "      <td>3A4</td>\n",
       "      <td>MAYLYGTHSHGLFKKLGIPGPTPLPFLGNILSYHKGFCMFDMECHK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paclitaxel</td>\n",
       "      <td>CC1=C2C(C(=O)C3(C(CC4C(C3C(C(C2(C)C)(CC1OC(=O)...</td>\n",
       "      <td>2C8</td>\n",
       "      <td>MAKKTSSKGKLPPGPTPLPIIGNMLQIDVKDICKSFTNFSKVYGPV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Amodiaquine</td>\n",
       "      <td>CCN(CC)CC1=C(C=CC(=C1)NC2=C3C=CC(=CC3=NC=C2)Cl)O</td>\n",
       "      <td>2C8</td>\n",
       "      <td>MAKKTSSKGKLPPGPTPLPIIGNMLQIDVKDICKSFTNFSKVYGPV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S-Warfarin</td>\n",
       "      <td>CC(=O)CC(C1=CC=CC=C1)C2=C(C3=CC=CC=C3OC2=O)O</td>\n",
       "      <td>2C9</td>\n",
       "      <td>MAKKTSGRGKLPPGPTPLPVIGNILQIGIKDISKSLTNLSKVYGPV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Diclofenac</td>\n",
       "      <td>C1=CC=C(C(=C1)CC(=O)O)NC2=C(C=CC=C2Cl)Cl</td>\n",
       "      <td>2C9</td>\n",
       "      <td>MAKKTSGRGKLPPGPTPLPVIGNILQIGIKDISKSLTNLSKVYGPV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>S-Mephenytoin</td>\n",
       "      <td>CCC1(C(=O)N(C(=O)N1)C)C2=CC=CC=C2</td>\n",
       "      <td>2C19</td>\n",
       "      <td>TIKEMPQPKTFGELKNLPLLNTDKPVQALMKIADELGEIFKFEAPG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bufuralol</td>\n",
       "      <td>CCC1=CC=CC2=C1OC(=C2)C(CNC(C)(C)C)O</td>\n",
       "      <td>2D6</td>\n",
       "      <td>MAKKTSSKGKLPPGPLPLPGLGNLLHVDFQNTPYCFDQLRRRFGDV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dextromethorphan</td>\n",
       "      <td>CN1CCC23CCCCC2C1CC4=C3C=C(C=C4)OC</td>\n",
       "      <td>2D6</td>\n",
       "      <td>MAKKTSSKGKLPPGPLPLPGLGNLLHVDFQNTPYCFDQLRRRFGDV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Drug_name                                             SMILES  \\\n",
       "0          Phenacetin                            CCOC1=CC=C(C=C1)NC(=O)C   \n",
       "1   7-Ethoxyresorufin                CCOC1=CC2=C(C=C1)N=C3C=CC(=O)C=C3O2   \n",
       "2           Efavirenz      C1CC1C#CC2(C3=C(C=CC(=C3)Cl)NC(=O)O2)C(F)(F)F   \n",
       "3           Bupropion                CC(C(=O)C1=CC(=CC=C1)Cl)NC(C)(C)C\\n   \n",
       "4           Midazolam    CC1=NC=C2N1C3=C(C=C(C=C3)Cl)C(=NC2)C4=CC=CC=C4F   \n",
       "5        Testosterone                CC12CCC3C(C1CCC2O)CCC4=CC(=O)CCC34C   \n",
       "6          Paclitaxel  CC1=C2C(C(=O)C3(C(CC4C(C3C(C(C2(C)C)(CC1OC(=O)...   \n",
       "7         Amodiaquine   CCN(CC)CC1=C(C=CC(=C1)NC2=C3C=CC(=CC3=NC=C2)Cl)O   \n",
       "8          S-Warfarin       CC(=O)CC(C1=CC=CC=C1)C2=C(C3=CC=CC=C3OC2=O)O   \n",
       "9          Diclofenac           C1=CC=C(C(=C1)CC(=O)O)NC2=C(C=CC=C2Cl)Cl   \n",
       "10      S-Mephenytoin                  CCC1(C(=O)N(C(=O)N1)C)C2=CC=CC=C2   \n",
       "11          Bufuralol                CCC1=CC=CC2=C1OC(=C2)C(CNC(C)(C)C)O   \n",
       "12   Dextromethorphan                  CN1CCC23CCCCC2C1CC4=C3C=C(C=C4)OC   \n",
       "\n",
       "   Target_name                                              FASTA  \n",
       "0          1A2  MAVLKGLRPRVPKGLKSPPEPWGWPLLGHVLTLGKNPHLALSRMSQ...  \n",
       "1          1A2  MAVLKGLRPRVPKGLKSPPEPWGWPLLGHVLTLGKNPHLALSRMSQ...  \n",
       "2          2B6  MAKKTSSKGKLPPGPRPLPLLGNLLQMDRRGLLKSFLRFREKYGDV...  \n",
       "3          2B6  MAKKTSSKGKLPPGPRPLPLLGNLLQMDRRGLLKSFLRFREKYGDV...  \n",
       "4          3A4  MAYLYGTHSHGLFKKLGIPGPTPLPFLGNILSYHKGFCMFDMECHK...  \n",
       "5          3A4  MAYLYGTHSHGLFKKLGIPGPTPLPFLGNILSYHKGFCMFDMECHK...  \n",
       "6          2C8  MAKKTSSKGKLPPGPTPLPIIGNMLQIDVKDICKSFTNFSKVYGPV...  \n",
       "7          2C8  MAKKTSSKGKLPPGPTPLPIIGNMLQIDVKDICKSFTNFSKVYGPV...  \n",
       "8          2C9  MAKKTSGRGKLPPGPTPLPVIGNILQIGIKDISKSLTNLSKVYGPV...  \n",
       "9          2C9  MAKKTSGRGKLPPGPTPLPVIGNILQIGIKDISKSLTNLSKVYGPV...  \n",
       "10        2C19  TIKEMPQPKTFGELKNLPLLNTDKPVQALMKIADELGEIFKFEAPG...  \n",
       "11         2D6  MAKKTSSKGKLPPGPLPLPGLGNLLHVDFQNTPYCFDQLRRRFGDV...  \n",
       "12         2D6  MAKKTSSKGKLPPGPLPLPGLGNLLHVDFQNTPYCFDQLRRRFGDV...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_no = 0\n",
    "max_length = 256\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizer, RobertaTokenizer\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "df = pd.read_excel(\"data/validation.xlsx\", engine=\"openpyxl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20860e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, mol_tokenizer, prot_tokenizer, device_no, prot_max_length=256):\n",
    "        self.prot_tokenizer = prot_tokenizer\n",
    "        self.mol_tokenizer = mol_tokenizer\n",
    "        self.prot_encoder = AutoModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "        self.prot_encoder.eval()\n",
    "        \n",
    "        self.device_no = device_no\n",
    "        self.prot_max_length = prot_max_length\n",
    "        \n",
    "    def encode_fasta(self, fasta):\n",
    "        target_seq = self.prot_tokenizer(\" \".join(fasta), max_length=self.prot_max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        return target_seq\n",
    "    \n",
    "    def encode_smiles(self, smiles):\n",
    "        drug_seq = self.mol_tokenizer(smiles, max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        return drug_seq\n",
    "    \n",
    "    def get_hint(self, fasta):\n",
    "        target_seq = self.encode_fasta(fasta)\n",
    "        hint = self.prot_encoder(**target_seq)\n",
    "        hint = hint.last_hidden_state.detach().to(\"cpu\")\n",
    "        \n",
    "        return hint[:, 0]\n",
    "    \n",
    "    def get_feat(self, fasta, smiles):\n",
    "        hint = self.get_hint(fasta)\n",
    "        target_seq = self.encode_fasta(fasta)\n",
    "        drug_seq = self.encode_smiles(smiles)\n",
    "        \n",
    "        return hint, target_seq, drug_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e86661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def load_tokenizer():\n",
    "    prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "    mol_tokenizer = RobertaTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "    return prot_tokenizer, mol_tokenizer\n",
    "\n",
    "prot_tokenizer, mol_tokenizer = load_tokenizer()\n",
    "mol_encoder = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=prot_tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=2048,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=max_length + 2,\n",
    "    type_vocab_size=1,\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "prot_encoder = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dca0609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DTI(\n",
       "  (mol_encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(767, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (prot_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(258, 512)\n",
       "      (token_type_embeddings): Embedding(1, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (molecule_align): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=768, out_features=512, bias=False)\n",
       "  )\n",
       "  (protein_align_teacher): Sequential(\n",
       "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=1024, out_features=512, bias=False)\n",
       "  )\n",
       "  (protein_align_student): Sequential(\n",
       "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (cls_out): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DTI(nn.Module):\n",
    "    def __init__(self, mol_encoder, prot_encoder, \n",
    "                 hidden_dim=512, mol_dim=128, prot_dim=1024):\n",
    "        super().__init__()\n",
    "        self.mol_encoder = mol_encoder\n",
    "        self.prot_encoder = prot_encoder\n",
    "        \n",
    "        self.lambda_ = torch.nn.Parameter(torch.rand(1).to(f\"cuda:{device_no}\"), requires_grad=True)\n",
    "                    \n",
    "        self.molecule_align = nn.Sequential(\n",
    "            nn.LayerNorm(mol_dim),\n",
    "            nn.Linear(mol_dim, hidden_dim, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.protein_align_teacher = nn.Sequential(\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Linear(1024, hidden_dim, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.protein_align_student = nn.Sequential(\n",
    "            nn.LayerNorm(prot_dim),\n",
    "            nn.Linear(prot_dim, hidden_dim, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        self.cls_out = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, SMILES, FASTA, prot_feat_teacher):\n",
    "        mol_feat = self.mol_encoder(**SMILES).last_hidden_state[:, 0]\n",
    "        prot_feat = self.prot_encoder(**FASTA).last_hidden_state[:, 0]\n",
    "        \n",
    "        mol_feat = self.molecule_align(mol_feat)\n",
    "        prot_feat = self.protein_align_student(prot_feat)\n",
    "        prot_feat_teacher = self.protein_align_teacher(prot_feat_teacher).squeeze(1)\n",
    "        \n",
    "        lambda_ = torch.sigmoid(self.lambda_)\n",
    "        merged_prot_feat = lambda_ * prot_feat + (1 - lambda_) * prot_feat_teacher\n",
    "    \n",
    "        x = torch.cat([mol_feat, merged_prot_feat], dim=1)\n",
    "\n",
    "        x = F.dropout(F.gelu(self.fc1(x)), 0.1)\n",
    "        x = F.dropout(F.gelu(self.fc2(x)), 0.1)\n",
    "        x = F.dropout(F.gelu(self.fc3(x)), 0.1)\n",
    "        \n",
    "        cls_out = self.cls_out(x).squeeze(-1)\n",
    "        \n",
    "        return cls_out, lambda_\n",
    "        \n",
    "model = DTI(mol_encoder, prot_encoder,\n",
    "            hidden_dim=512, mol_dim=768, prot_dim=512)\n",
    "\n",
    "model.load_state_dict(torch.load(\"weights/DLM_DTI_prot-256_.pt\"))\n",
    "model = model.to(f\"cuda:{device_no}\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44753d0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9986], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([1.0000], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9997], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9987], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9930], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9996], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9972], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9993], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9999], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9994], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.8251], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([1.0000], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([1.0000], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "preprocessor = Preprocessor(mol_tokenizer, prot_tokenizer, device_no, prot_max_length=256)\n",
    "\n",
    "time_list = []\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    start = time.time()\n",
    "    hint, target_seq, drug_seq = preprocessor.get_feat(df.loc[i, \"FASTA\"], df.loc[i, \"SMILES\"])\n",
    "    prob, _ = model(drug_seq.to(f\"cuda:{device_no}\"), target_seq.to(f\"cuda:{device_no}\"), hint.to(f\"cuda:{device_no}\"))\n",
    "    end = time.time()\n",
    "    \n",
    "    time_list.append(end - start)\n",
    "    print(F.sigmoid(prob))\n",
    "    \n",
    "time_list = np.array(time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1879b40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35838868067814755, 0.004643884170554766)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(time_list), np.std(time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeabd68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
